{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnavey/CALVADOS_poly/blob/main/RealKcat_Inference_Interface_Class_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPpF3-QZGKjT"
      },
      "source": [
        "**Robust Prediction of Enzyme Variant Kinetics with RealKcat**\n",
        "\n",
        "This notebook predicts enzyme kinetic parameters ranges ($k_{\\text{cat}}$ and $K_M$) for given protein sequences and substrates using the RealKcat model, which leverages enzyme amino acid sequences and Isomeric SMILEs for substrates.\n",
        "\n",
        "_No coding experience required. Collapse cells to keep the notebook clean._  \n",
        "\n",
        "---\n",
        "\n",
        "## **How to Use**\n",
        "1. **Select Mode**: `Demo`, `Interactive`, `Bulk`, or `Bulk-large`  \n",
        "2. **Select Mutation Pathway**:  \n",
        "   - **Mechanistic Mutation-Aware (default)** – realistic graded mutation effects (recommended)  \n",
        "   - **Binary Over-Simplified (poor and optional)** – legacy binary alanine handling for benchmarking only  \n",
        "3. **Run the cells sequentially below**  \n",
        "4. **Follow on-screen prompts** (enter input or upload CSV)  \n",
        "5. **Download results** (CSV is auto-saved to your computer)  \n",
        "\n",
        "> ⚠️ Each time you run the inference cell, Colab will:  \n",
        "> - Install required dependencies  \n",
        "> - Download pretrained model files (~2 minutes setup)  \n",
        "\n",
        "---\n",
        "\n",
        "## **Modes**\n",
        "- **`Demo`** → Test run with predefined inputs (~2 min)  \n",
        "- **`Interactive`** → Manually enter up to 10 enzyme–substrate pairs  \n",
        "- **`Bulk`** → Upload CSV with ≤10 pairs  \n",
        "  - [Sample CSV](https://drive.google.com/uc?export=download&id=1X9bR67NW-sTNHaKU4W1Htlfxhl-OKMmu)  \n",
        "- **`Bulk-large`** → For >10 pairs (batch size = 20 by default, could be slower in speed. GPU recommended).  \n",
        "  - Outputs extended results including ±1-class error (“e-accuracy”), spanning one order of magnitude around predictions to reflect physiological robustness (pH, cofactors, _in vitro_ vs _in vivo_).  \n",
        "  - **If your sequences are long (~>500 aa), start with a very small BATCH_SIZE (1–2) to avoid running out of RAM in Google Colab, then increase gradually.**\n",
        "\n",
        "---\n",
        "\n",
        "## **Limits**\n",
        "- Maximum sequence length: **1022 amino acids**  \n",
        "- Maximum SMILES length: **512 characters**  \n",
        "- Entries exceeding these will be marked as `\"skipped\"` in the output.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "XGMrWhdZPAs9",
        "outputId": "e4dcdd15-dda8-4ad5-d4a8-bd9e89de45a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7a0e0edd-021d-437e-8735-d28d48431265\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7a0e0edd-021d-437e-8735-d28d48431265\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Select RealKcat Mode of Inference,  $k_{cat}$ and $K_{M}$  [Demo  takes ~4-10 minutes]\n",
        "mode = \"Bulk-large\"  #@param [\"Demo\", \"Interactive\", \"Bulk\", \"Bulk-large\"]\n",
        "#@title Select mutation handling pathway\n",
        "mutation_mode = \"Mechanistic Mutation-Aware (default)\"  #@param [\"Mechanistic Mutation-Aware (default)\", \"Binary Over-Simplified (poor and optional)\"]\n",
        "import pandas as pd, io, os, sys\n",
        "CSV_OUT = \"infer_input.csv\"\n",
        "BATCH_SIZE = 20\n",
        "try:\n",
        "    from google.colab import files as _colab_files\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    _colab_files = None\n",
        "    IS_COLAB = False\n",
        "def _clean(df):\n",
        "    df = df.copy()\n",
        "    df[\"sequence\"] = df[\"sequence\"].astype(str).str.replace(r\"\\s+\", \"\", regex=True)\n",
        "    df[\"Isomeric SMILES\"] = df[\"Isomeric SMILES\"].astype(str).str.replace(r\"\\s+\", \"\", regex=True)\n",
        "    return df\n",
        "if mode == \"Bulk\":\n",
        "    if IS_COLAB:\n",
        "        uploaded = _colab_files.upload()\n",
        "        if not uploaded: raise RuntimeError(\"no file\")\n",
        "        f = list(uploaded.keys())[0]\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[f]))\n",
        "    else:\n",
        "        p = input(\"Path to CSV with 'sequence' and 'Isomeric SMILES': \").strip()\n",
        "        if not p: raise RuntimeError(\"no path\")\n",
        "        df = pd.read_csv(p)\n",
        "    df = _clean(df)\n",
        "    df.to_csv(CSV_OUT, index=False)\n",
        "elif mode == \"Bulk-large\":\n",
        "    if IS_COLAB:\n",
        "        uploaded = _colab_files.upload()\n",
        "        if not uploaded: raise RuntimeError(\"no file\")\n",
        "        f = list(uploaded.keys())[0]\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[f]))\n",
        "    else:\n",
        "        p = input(\"Path to CSV with 'sequence' and 'Isomeric SMILES': \").strip()\n",
        "        if not p: raise RuntimeError(\"no path\")\n",
        "        df = pd.read_csv(p)\n",
        "    df = _clean(df)\n",
        "    df.to_csv(CSV_OUT, index=False)\n",
        "    try:\n",
        "        s = input(\"Batch size (default 20): \").strip()\n",
        "        if s: BATCH_SIZE = int(s)\n",
        "    except Exception:\n",
        "        pass\n",
        "elif mode in (\"Interactive\", \"Demo\"):\n",
        "    pairs = []\n",
        "    if mode == \"Demo\":\n",
        "        pairs = [\n",
        "            (\"MKVAVLGAAGGIGQALALLLKTQLPSGSELSLYDIAPVTPGVAVDLSHIPTAVKIKGFSGEDATPALEGADVVLISAGVARKPGMDRSDLFNVNAGIVKNLVQQVAKTCPKACIGIITNPVNTTVAIAAEVLKKAGVYDKNKLFGVTTLDIIRSNTFVAELKGKQPGEVEVPVIGGHSGVTILPLLSQVPGVSFTEQEVADLTKRIQNAGTEVVEAKAGGGSATLSMGQAAARFGLSLVRALQGEQGVVECAYVEGDGQYARFFSQPLLLGKNGVEERKSIGTLSAFEQNALEGMLDTLKKDIALGEEFVNK\",\"C(C(=O)C(=O)O)C(=O)O\"),\n",
        "            (\"MGVEQILKRKTGVIVGEDVHNLFTYAKEHKFAIPAINVTSSSTAVAALEAARDSKSPIILQTSNGGAAYFAGKGISNEGQNASIKGAIAAAHYIRSIAPAYGIPVVLHSDHCAKKLLPWFDGMLEADEAYFKEHGEPLFSSHMLDLSEETDEENISTCVKYFKRMAAMDQWLEMEIGITGGEEDGVNNENADKEDLYTKPEQVYNVYKALHPISPNFSIAAAFGNCHGLYAGDIALRPEILAEHQKYTREQVGCKEEKPLFLVFHGGSGSTVQEFHTGIDNGVVKVNLDTDCQYAYLTGIRDYVLNKKDYIMSPVGNPEGPEKPNKKFFDPRVWVREGEKTMGAKITKSLETFRTTNTL\",\"C([C@H](C=O)O)OP(=O)(O)O\"),\n",
        "            (\"M\"*1023,\"C(C=O)\"),\n",
        "            (\"MKKVAV\",\"C\"+\"C\"*512),\n",
        "        ]\n",
        "    else:\n",
        "        MAX=10\n",
        "        while len(pairs)<MAX:\n",
        "            s=input(f\"Sequence #{len(pairs)+1} (blank to stop): \").strip()\n",
        "            if not s: break\n",
        "            m=input(\"Isomeric SMILES: \").strip()\n",
        "            if not m: continue\n",
        "            pairs.append((s,m))\n",
        "    if not pairs: raise RuntimeError(\"no inputs\")\n",
        "    df = pd.DataFrame(pairs, columns=[\"sequence\",\"Isomeric SMILES\"])\n",
        "    df = _clean(df)\n",
        "    df.to_csv(CSV_OUT, index=False)\n",
        "else:\n",
        "    raise ValueError(mode)\n",
        "from IPython import get_ipython\n",
        "ip = get_ipython()\n",
        "ip.run_line_magic(\"env\", f\"MODE={mode}\")\n",
        "ip.run_line_magic(\"env\", f\"CSV_PATH={os.path.abspath(CSV_OUT)}\")\n",
        "ip.run_line_magic(\"env\", f\"BATCH_SIZE={BATCH_SIZE}\")\n",
        "ip.run_line_magic(\"env\", f\"MUTATION_MODE={mutation_mode}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYPHkhPUK-lC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f1225f-71c3-43b3-fb67-c95a5f91251f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] MODE=Bulk-large  MUTATION_MODE=Mechanistic Mutation-Aware (default)  CSV_PATH=/content/infer_input.csv  BATCH_SIZE=256\n",
            "[RealKcat] Running Mechanistic Mutation-Aware mode (recommended, ESM-C)...\n",
            "[setup] Installing system deps for Python 3.12…\n",
            "[setup] Installing Python deps…\n",
            "[run] Starting mechanistic inference… MODE=Bulk-large  CSV_PATH=/content/infer_input.csv  BATCH_SIZE=256  ENCODER=esmc  ALLOW_ENCODER_MISMATCH=1\n",
            "Using device: cuda\n",
            "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Starting inference setup…\n",
            "\rBatches:   0%|          | 0/20 [00:00<?, ?batch/s][ESMC] Weights missing; downloading from HuggingFace…\n",
            "\rBatches:   5%|▌         | 1/20 [03:22<1:04:03, 202.30s/batch]\rBatches:  10%|█         | 2/20 [04:18<34:59, 116.64s/batch]  \rBatches:  15%|█▌        | 3/20 [05:14<25:08, 88.74s/batch] \rBatches:  20%|██        | 4/20 [06:10<20:12, 75.79s/batch]\rBatches:  25%|██▌       | 5/20 [07:06<17:08, 68.57s/batch]\rBatches:  30%|███       | 6/20 [08:01<14:57, 64.09s/batch]\rBatches:  35%|███▌      | 7/20 [08:57<13:17, 61.35s/batch]\rBatches:  40%|████      | 8/20 [09:52<11:52, 59.41s/batch]\rBatches:  45%|████▌     | 9/20 [10:48<10:40, 58.22s/batch]\rBatches:  50%|█████     | 10/20 [11:43<09:32, 57.27s/batch]\rBatches:  55%|█████▌    | 11/20 [12:38<08:30, 56.75s/batch]\rBatches:  60%|██████    | 12/20 [13:34<07:29, 56.25s/batch]\rBatches:  65%|██████▌   | 13/20 [14:30<06:33, 56.17s/batch]\rBatches:  70%|███████   | 14/20 [15:25<05:35, 55.90s/batch]\rBatches:  75%|███████▌  | 15/20 [16:21<04:39, 55.98s/batch]\rBatches:  80%|████████  | 16/20 [17:16<03:43, 55.82s/batch]\rBatches:  85%|████████▌ | 17/20 [18:12<02:47, 55.82s/batch]\rBatches:  90%|█████████ | 18/20 [19:08<01:51, 55.73s/batch]\rBatches:  95%|█████████▌| 19/20 [19:59<00:54, 54.33s/batch]\rBatches: 100%|██████████| 20/20 [20:27<00:00, 46.48s/batch]\rBatches: 100%|██████████| 20/20 [20:27<00:00, 61.37s/batch]\n",
            "Inference complete. Saved inference_results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tobler 0.13.0 requires joblib>=1.4, but you have joblib 1.2.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.0 which is incompatible.\n",
            "\n",
            "\rFetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "\rFetching 4 files:  25%|██▌       | 1/4 [00:00<00:01,  2.63it/s]\u001b[A\n",
            "\rFetching 4 files: 100%|██████████| 4/4 [00:52<00:00, 14.18s/it]\u001b[A\rFetching 4 files: 100%|██████████| 4/4 [00:52<00:00, 13.14s/it]\n",
            "\n",
            "\rFetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "\rFetching 4 files:  50%|█████     | 2/4 [00:00<00:00, 10.46it/s]\u001b[A\n",
            "\rFetching 4 files:  50%|█████     | 2/4 [00:13<00:00, 10.46it/s]\u001b[A\n",
            "\rFetching 4 files: 100%|██████████| 4/4 [00:59<00:00, 17.48s/it]\u001b[A\rFetching 4 files: 100%|██████████| 4/4 [00:59<00:00, 14.87s/it]\n",
            "2026-02-25 22:02:13.915720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1772056934.137583    2515 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1772056934.196408    2515 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1772056934.634150    2515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1772056934.634183    2515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1772056934.634187    2515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1772056934.634190    2515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-25 22:02:14.676501: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "#@title Run RealKcat Inference (Mechanistic or Binary, depending on selection above)\n",
        "%%bash\n",
        "set -e\n",
        "export DEBIAN_FRONTEND=noninteractive\n",
        "export TF_CPP_MIN_LOG_LEVEL=0   # <-- suppress TF INFO/WARNING/ERROR logs\n",
        "export XLA_FLAGS=\"--xla_cpu_enable_fast_math=false\"\n",
        "# pip uninstall -y tensorflow tensorflow-gpu tensorflow-cpu tensorflow-intel tf-nightly 2>/dev/null || true\n",
        "if [ -z \"${MODE:-}\" ]; then\n",
        "  echo \"No mode selected. Please run the cell above to choose a mode of inference first.\"\n",
        "  exit 1\n",
        "fi\n",
        ": \"${CSV_PATH:?}\"; : \"${BATCH_SIZE:=20}\"\n",
        "MUTATION_MODE=\"${MUTATION_MODE:-Mechanistic}\"\n",
        "echo \"[info] MODE=${MODE}  MUTATION_MODE=${MUTATION_MODE}  CSV_PATH=${CSV_PATH}  BATCH_SIZE=${BATCH_SIZE}\"\n",
        "\n",
        "# ---------------------------- MECHANISTIC BRANCH (ESM-C, v2-like) ----------------------------\n",
        "if [ \"${MUTATION_MODE}\" = \"Mechanistic Mutation-Aware (default)\" ]; then\n",
        "  echo \"[RealKcat] Running Mechanistic Mutation-Aware mode (recommended, ESM-C)...\"\n",
        "  # optional clean-up\n",
        "  # pip uninstall -y tensorflow tensorflow-gpu tensorflow-cpu tensorflow-intel tf-nightly 2>/dev/null || true\n",
        "  echo \"[setup] Installing system deps for Python 3.12…\"\n",
        "  sudo sed -i 's/^[[:space:]]*deb-src .*r2u\\.stat\\.illinois\\.edu.*$/# &/' /etc/apt/sources.list /etc/apt/sources.list.d/*.list 2>/dev/null || true\n",
        "  sudo apt-get update -qq > /dev/null\n",
        "  sudo apt-get install -qq -y python3.12 python3.12-venv python3-distutils curl wget unzip > /dev/null\n",
        "  curl -sS https://bootstrap.pypa.io/get-pip.py -o /tmp/get-pip.py\n",
        "  python3.12 /tmp/get-pip.py --quiet\n",
        "  echo \"[setup] Installing Python deps…\"\n",
        "  python3.12 -m pip install -q \\\n",
        "    numpy==2.1 pandas==2.2.2 scikit-learn==1.7.1 imbalanced-learn==0.8.1 \\\n",
        "    seaborn==0.11.2 joblib==1.2.0 ipython==7.34.0 \\\n",
        "    matplotlib==3.10.5 \\\n",
        "    notebook==6.5.4 jupyterlab==3.6.1 openpyxl==3.1.2 xlrd==2.0.1 XlsxWriter==3.0.3\n",
        "  python3.12 -m pip install -q \\\n",
        "    xgboost==2.1.4 torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 \\\n",
        "    tqdm esm==3.2.3 httpx==0.28.1 biotite==1.2.0 \\\n",
        "    # huggingface_hub==1.2.3 #transformers==4.46.3  \\\n",
        "    huggingface_hub==0.25.2 #transformers==4.46.3  \\\n",
        "  : > latex_queue.txt\n",
        "  echo \"[run] Starting mechanistic inference… MODE=${MODE}  CSV_PATH=${CSV_PATH}  BATCH_SIZE=${BATCH_SIZE}  ENCODER=${ENCODER:-esmc}  ALLOW_ENCODER_MISMATCH=${ALLOW_ENCODER_MISMATCH:-1}\"\n",
        "  MODE=\"$MODE\" CSV_PATH=\"$CSV_PATH\" BATCH_SIZE=\"$BATCH_SIZE\" stdbuf -oL -eL python3.12 - <<'PY'\n",
        "import os, sys, warnings, torch, argparse, random, math, joblib, numpy as np, pandas as pd, io, gc\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import logging\n",
        "logging.disable(logging.CRITICAL)\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.serialization import add_safe_globals\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import snapshot_download\n",
        "from esm.models.esmc import ESMC\n",
        "from esm.sdk.api import ESMProtein, LogitsConfig\n",
        "# ---------------- basic runtime ----------------\n",
        "os.environ[\"OMP_NUM_THREADS\"]=\"1\"; os.environ[\"MKL_NUM_THREADS\"]=\"1\"\n",
        "try:\n",
        "    torch.set_num_threads(1)\n",
        "except Exception:\n",
        "    pass\n",
        "MODE = os.environ.get(\"MODE\",\"Demo\").strip()\n",
        "CSV_PATH = os.environ[\"CSV_PATH\"]\n",
        "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\",\"20\"))\n",
        "ENCODER = os.environ.get(\"ENCODER\",\"esmc\").strip().lower()          # only \"esmc\" supported here\n",
        "ALLOW_ENCODER_MISMATCH = int(os.environ.get(\"ALLOW_ENCODER_MISMATCH\",\"1\"))\n",
        "try:\n",
        "    from google.colab import files as _cf\n",
        "    IS_COLAB=True\n",
        "except Exception:\n",
        "    _cf=None; IS_COLAB=False\n",
        "def check_device(): return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = check_device()\n",
        "print(\"Using device:\", device); sys.stdout.flush()\n",
        "print(sys.version); sys.stdout.flush()\n",
        "print(\"Starting inference setup…\"); sys.stdout.flush()\n",
        "MAX_SEQ_LENGTH=1022\n",
        "MAX_SMILES_LENGTH=512\n",
        "# ---------------- class ranges ----------------\n",
        "#  Build class ranges for kcat and km\n",
        "kcat_log_bins = np.array([0, 1e-08, 1e-02, 1e-01, 1e+00, 1e+01, 1e+02, 1e+03, 1e+08])\n",
        "km_log_bins   = np.array([1e-14, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 1e+04])\n",
        "class_ranges_kcat = {i: {\"low\": float(kcat_log_bins[i]), \"high\": float(kcat_log_bins[i+1])}\n",
        "                for i in range(len(kcat_log_bins)-1)}\n",
        "class_ranges_km = {i: {\"low\": float(km_log_bins[i]), \"high\": float(km_log_bins[i+1])}\n",
        "                for i in range(len(km_log_bins)-1)}\n",
        "def format_sci(v):\n",
        "    s=f\"{v:.2e}\"\n",
        "    if \"e\" in s:\n",
        "        b,e=s.split(\"e\"); return f\"{b}x10^{int(e)}\"\n",
        "    return s\n",
        "warnings.filterwarnings(\"ignore\"); add_safe_globals([argparse.Namespace])\n",
        "# ---------------- data load & basic cleaning ----------------\n",
        "df=pd.read_csv(CSV_PATH)\n",
        "df[\"sequence\"]=df[\"sequence\"].astype(str).str.replace(r\"\\s+\",\"\",regex=True).str.upper() \\\n",
        "            .str.replace(r\"[^ACDEFGHIKLMNPQRSTVWY]\", \"X\", regex=True)\n",
        "df[\"Isomeric SMILES\"]=df[\"Isomeric SMILES\"].astype(str).str.replace(r\"\\s+\",\"\",regex=True)\n",
        "pairs=list(zip(df[\"sequence\"],df[\"Isomeric SMILES\"]))\n",
        "# ---------------- ESM-C loader & embed ----------------\n",
        "def load_esmc_model(device, work_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Loads ESM-C 600M to work_dir/esmc_model/... if needed, then returns eval() model.\n",
        "    \"\"\"\n",
        "    repo_id = \"EvolutionaryScale/esmc-600m-2024-12\"\n",
        "    local_dir = os.path.join(work_dir, \"esmc_model\")\n",
        "    weights_rel = \"data/weights/esmc_600m_2024_12_v0.pth\"\n",
        "    weights_path = os.path.join(local_dir, weights_rel)\n",
        "    if not os.path.exists(weights_path):\n",
        "        print(\"[ESMC] Weights missing; downloading from HuggingFace…\")\n",
        "        from huggingface_hub import logging as hf_logging\n",
        "        hf_logging.set_verbosity_error()\n",
        "        os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "        snapshot_download(repo_id=repo_id, local_dir=local_dir, force_download=False,tqdm_class=None)\n",
        "    model = ESMC.from_pretrained(\"esmc_600m\").to(device)\n",
        "    if os.path.exists(weights_path):\n",
        "        try:\n",
        "            sd = torch.load(weights_path, map_location=device)\n",
        "            model.load_state_dict(sd, strict=False)\n",
        "        except Exception as e:\n",
        "            print(f\"[ESMC] Warning: could not load local weights strictly: {e}\")\n",
        "    model.eval()\n",
        "    return model\n",
        "@torch.no_grad()\n",
        "def esmc_mean_embed(model, sequence: str, device):\n",
        "    \"\"\"\n",
        "    Returns mean-pooled embedding over residues (excl. special tokens).\n",
        "    \"\"\"\n",
        "    protein = ESMProtein(sequence=sequence)\n",
        "    toks = model.encode(protein)  # (B, L)\n",
        "    out = model.logits(toks, LogitsConfig(sequence=True, structure=True, return_embeddings=True))\n",
        "    L = len(sequence)\n",
        "    reps = out.embeddings[0, 1:L-1]  # drop [CLS]/[EOS]-like\n",
        "    return reps.mean(dim=0).float().to(device)  # (D_esmc,)\n",
        "# ---------------- chem encoder (unchanged) ----------------\n",
        "_ESMC=None; _TOK=None; _CHEM=None\n",
        "def _get_models():\n",
        "    global _ESMC,_TOK,_CHEM\n",
        "    if ENCODER != \"esmc\":\n",
        "        raise RuntimeError(\"This cell implements only ENCODER=esmc. (esm2 path omitted.)\")\n",
        "    if _ESMC is None:\n",
        "        _ESMC = load_esmc_model(device)\n",
        "    if _TOK is None or _CHEM is None:\n",
        "        _TOK=AutoTokenizer.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k')\n",
        "        _CHEM=AutoModel.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k'); _CHEM.eval().to(device)\n",
        "    return _ESMC, _TOK, _CHEM\n",
        "# ---------------- dataset/standardization (unchanged split) ----------------\n",
        "class TensorDataset(Dataset):\n",
        "    def __init__(self,d,l): self.d,self.l=d,l\n",
        "    def __len__(self): return len(self.d)\n",
        "    def __getitem__(self,i): return self.d[i],self.l[i]\n",
        "def dataset_to_tensors(ds):\n",
        "    ld=DataLoader(ds,batch_size=len(ds),shuffle=False); return next(iter(ld))\n",
        "def standardize_x_global_separate(data,g1,s1,g2,s2):\n",
        "    # legacy split: first 1280 dims are sequence, remainder is chem\n",
        "    # X1,X2=data[:,:1280],data[:,1280:]          # ESM-2\n",
        "    X1,X2 = data[:, :1152], data[:, 1152:]       # ESM-C\n",
        "    s1=torch.clamp(s1,min=1e-7); s2=torch.clamp(s2,min=1e-7)\n",
        "    return torch.cat(((X1-g1)/s1,(X2-g2)/s2),dim=1).squeeze(1)\n",
        "class StandardizedDatasetGlobalSeparate(Dataset):\n",
        "    def __init__(self,sub,g1,s1,g2,s2): self.sub=sub; self.g1=g1; self.s1=s1; self.g2=g2; self.s2=s2\n",
        "    def __len__(self): return len(self.sub)\n",
        "    def __getitem__(self,i):\n",
        "        x,y=self.sub[i]\n",
        "        if len(x.shape)==1: x=x.unsqueeze(1)\n",
        "        return standardize_x_global_separate(x,self.g1,self.s1,self.g2,self.s2),y\n",
        "def apply_global_standardization_separate(ds,g1,s1,g2,s2):\n",
        "    return StandardizedDatasetGlobalSeparate(ds,g1,s1,g2,s2)\n",
        "# legacy global stats for sequence (1152) and chem block\n",
        "global_mean_1=torch.tensor(-0.0004980484955012798,device=device)\n",
        "global_std_1=torch.tensor(0.027508573606610298,device=device)\n",
        "global_mean_2=torch.tensor(-2.7928688723477535e-05,device=device)\n",
        "global_std_2=torch.tensor(0.6221200823783875,device=device)\n",
        "# ---------------- model weights (unchanged) ----------------\n",
        "kcat_model_v1b_path=\"model_weights/kcat_model_v1b.pkl\"\n",
        "km_model_v1b_path=\"model_weights/km_model_v1b.pkl\"\n",
        "if not (os.path.exists(kcat_model_v1b_path) and os.path.exists(km_model_v1b_path)):\n",
        "    os.system(\"wget -q https://github.com/TKAI-LAB-Mali/RealKcat/raw/main/model_weights.zip -O model_weights.zip\")\n",
        "    os.system(\"unzip -o -q model_weights.zip\")\n",
        "# ---------------- inference wrapper ----------------\n",
        "class KcatInference:\n",
        "    def __init__(self,model_path,device=None,verbose=True):\n",
        "        self.device=device if device else device\n",
        "        self.model=joblib.load(model_path); self.verbose=verbose\n",
        "        self.X_test_tensor=None; self.y1_test_tensor=None; self.keep_local=[]\n",
        "    def load_data_from_pairs(self,pairs):\n",
        "        embs=[]; keep=[]\n",
        "        esmc_model, tok, chem = _get_models()\n",
        "        if self.verbose:\n",
        "            print(f\"Models loaded (ENCODER={ENCODER}). Embedding sequences and substrates...\"); sys.stdout.flush()\n",
        "        for i,(seq,smi) in enumerate(pairs,1):\n",
        "            if not isinstance(seq,str) or not isinstance(smi,str): continue\n",
        "            if len(seq)>MAX_SEQ_LENGTH: continue\n",
        "            # ---- sequence (ESM-C) ----\n",
        "            try:\n",
        "                es = esmc_mean_embed(esmc_model, seq, device)   # (D_esmc,)\n",
        "                # enforce legacy 1152-dim for downstream scaler/models\n",
        "                target_d = 1152\n",
        "                d = es.numel()\n",
        "                if d != target_d:\n",
        "                    if not ALLOW_ENCODER_MISMATCH:\n",
        "                        raise RuntimeError(\n",
        "                            f\"ESM-C embedding dim {d} != {target_d} expected by trained models. \"\n",
        "                            f\"Set ALLOW_ENCODER_MISMATCH=1 to pad/truncate (for plumbing only), \"\n",
        "                            f\"or retrain RealKcat with ESM-C.\"\n",
        "                        )\n",
        "                    es = es[:target_d] if d > target_d else torch.cat([es, torch.zeros(target_d - d, device=es.device)])\n",
        "            except Exception:\n",
        "                continue\n",
        "            # ---- substrate SMILES (PubChem tokenizer/encoder) ----\n",
        "            try:\n",
        "                inp=tok([smi],return_tensors='pt',padding=True,truncation=False)\n",
        "                inp={k:v.to(device) for k,v in inp.items()}\n",
        "                if inp['input_ids'].shape[1]>MAX_SMILES_LENGTH: continue\n",
        "                with torch.no_grad(): out=chem(**inp)\n",
        "                cs=out.last_hidden_state.mean(dim=1).squeeze(0).float()\n",
        "            except Exception:\n",
        "                continue\n",
        "            if es.dim()>1: es=es.flatten()\n",
        "            if cs.dim()>1: cs=cs.flatten()\n",
        "            embs.append(torch.cat((es,cs))); keep.append(i-1)\n",
        "        if not embs:\n",
        "            self.X_test_tensor=None; self.y1_test_tensor=None; self.keep_local=[]\n",
        "            return\n",
        "        self.X_test_tensor=torch.stack(embs).to(device)\n",
        "        self.y1_test_tensor=torch.zeros(len(embs),dtype=torch.long).to(device)\n",
        "        self.keep_local=keep\n",
        "    def standardize_test_data(self,g1,s1,g2,s2):\n",
        "        self.test_dataset_std=apply_global_standardization_separate(\n",
        "            TensorDataset(self.X_test_tensor,self.y1_test_tensor),g1,s1,g2,s2\n",
        "        ) if self.X_test_tensor is not None else None\n",
        "    def convert_to_numpy(self):\n",
        "        if self.test_dataset_std is None: return None,None,[]\n",
        "        X,y=dataset_to_tensors(self.test_dataset_std); return X.cpu().numpy(),y.cpu().numpy(),self.keep_local\n",
        "    def predict(self,X): return self.model.predict(X)\n",
        "    def display_prediction_ranges_kcat(self,preds,cr):\n",
        "        # section added to analyze preds - by mariana\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"display_prediction_ranges_kcat: preds parameter contents\")\n",
        "        print(f\"Type of preds: {type(preds)}\")\n",
        "        print(f\"Length of preds: {len(preds)}\")\n",
        "        print(f\"First 10 elements: {preds[:10]}\")\n",
        "        print(f\"Full preds list: {preds}\")\n",
        "        print(preds)\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "        # end of section\n",
        "        for i,p in enumerate(preds):\n",
        "            if p is None or p==\"skipped\": print(f\"Sample {i+1}: skipped due to excessive length\")\n",
        "            else: print(f\"Sample {i+1}: Predicted Class = {p}, kcat range = [{format_sci(cr[p]['low'])}, {format_sci(cr[p]['high'])}]\")\n",
        "    def display_prediction_ranges_km(self,preds,cr):\n",
        "        # section added to analyze preds - by mariana\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"display_prediction_ranges_km: preds parameter contents\")\n",
        "        print(f\"Type of preds: {type(preds)}\")\n",
        "        print(f\"Length of preds: {len(preds)}\")\n",
        "        print(f\"First 10 elements: {preds[:10]}\")\n",
        "        print(f\"Full preds list: {preds}\")\n",
        "        print(preds)\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "        # end of section\n",
        "        for i,p in enumerate(preds):\n",
        "            if p is None or p==\"skipped\": print(f\"Sample {i+1}: skipped due to excessive length\")\n",
        "            else: print(f\"Sample {i+1}: Predicted Class = {p}, km range = [{format_sci(cr[p]['low'])}, {format_sci(cr[p]['high'])}]\")\n",
        "# ---------------- driver ----------------\n",
        "pairs_all=pairs\n",
        "valid=[i for i,(s,m) in enumerate(pairs_all)\n",
        "      if isinstance(s,str) and isinstance(m,str)\n",
        "      and len(s)<=MAX_SEQ_LENGTH and len(m)<=MAX_SMILES_LENGTH]\n",
        "if not valid:\n",
        "    df[\"Predicted_Kcat_low\"]=\"skipped\"; df[\"Predicted_Kcat_high\"]=\"skipped\"\n",
        "    df[\"Predicted_KM_low\"]=\"skipped\"; df[\"Predicted_KM_high\"]=\"skipped\"\n",
        "    df.to_csv(\"inference_results.csv\",index=False)\n",
        "    print(\"Inference complete. Saved inference_results.csv\")\n",
        "    if MODE in (\"Bulk\",\"Bulk-large\") and IS_COLAB: _cf.download(\"inference_results.csv\")\n",
        "    sys.exit(0)\n",
        "def run_batch(idxs):\n",
        "    b=[pairs_all[i] for i in idxs]\n",
        "    inf=KcatInference(model_path=kcat_model_v1b_path,device=device,verbose=False)\n",
        "    inf.load_data_from_pairs(b)\n",
        "    if not inf.keep_local:\n",
        "        return [],[],[]\n",
        "    inf.standardize_test_data(global_mean_1,global_std_1,global_mean_2,global_std_2)\n",
        "    X,_,keep_local=inf.convert_to_numpy()\n",
        "    yk=KcatInference(model_path=kcat_model_v1b_path,device=device,verbose=False).predict(X)\n",
        "    ym=KcatInference(model_path=km_model_v1b_path,device=device,verbose=False).predict(X)\n",
        "    del inf, X; gc.collect()\n",
        "    return yk,ym,keep_local\n",
        "N=len(df)\n",
        "kcat_low_full=['skipped']*N; kcat_high_full=['skipped']*N; kcat_low_m1=['skipped']*N; kcat_high_p1=['skipped']*N\n",
        "km_low_full=['skipped']*N; km_high_full=['skipped']*N; km_low_m1=['skipped']*N; km_high_p1=['skipped']*N\n",
        "# added section for predicted class numbers - by mariana\n",
        "kcat_class_full=['skipped']*N\n",
        "km_class_full=['skipped']*N\n",
        "# end of added section\n",
        "max_kcat_c=max(class_ranges_kcat.keys()); max_km_c=max(class_ranges_km.keys())\n",
        "if MODE==\"Bulk-large\":\n",
        "    total_batches = math.ceil(len(valid)/BATCH_SIZE)\n",
        "    for b in tqdm(range(0, len(valid), BATCH_SIZE), desc=\"Batches\", unit=\"batch\", dynamic_ncols=True, file=sys.stdout, miniters=1):\n",
        "        idxs=valid[b:b+BATCH_SIZE]; yk,ym,keep=run_batch(idxs)\n",
        "        if not keep: continue\n",
        "        for j,k in enumerate(keep):\n",
        "            i0=idxs[k]; ck=int(yk[j]); cm=int(ym[j])\n",
        "            # added section to store class numbers - by mariana\n",
        "            kcat_class_full[i0]=ck\n",
        "            km_class_full[i0]=cm\n",
        "            # end of added section\n",
        "            kcat_low_full[i0]=class_ranges_kcat[ck][\"low\"]; kcat_high_full[i0]=class_ranges_kcat[ck][\"high\"]\n",
        "            kcat_low_m1[i0]=class_ranges_kcat[max(ck-1,0)][\"low\"]; kcat_high_p1[i0]=class_ranges_kcat[min(ck+1,max_kcat_c)][\"high\"]\n",
        "            km_low_full[i0]=class_ranges_km[cm][\"low\"]; km_high_full[i0]=class_ranges_km[cm][\"high\"]\n",
        "            km_low_m1[i0]=class_ranges_km[max(cm-1,0)][\"low\"]; km_high_p1[i0]=class_ranges_km[min(cm+1,max_km_c)][\"high\"]\n",
        "        gc.collect(); sys.stdout.flush()\n",
        "else:\n",
        "    yk,ym,keep=run_batch(valid)\n",
        "    if keep:\n",
        "        for j,k in enumerate(keep):\n",
        "            i0=valid[k]; ck=int(yk[j]); cm=int(ym[j])\n",
        "            # added section to store class numbers - by mariana\n",
        "            kcat_class_full[i0]=ck\n",
        "            km_class_full[i0]=cm\n",
        "            # end of added section\n",
        "            kcat_low_full[i0]=class_ranges_kcat[ck][\"low\"]; kcat_high_full[i0]=class_ranges_kcat[ck][\"high\"]\n",
        "            km_low_full[i0]=class_ranges_km[cm][\"low\"]; km_high_full[i0]=class_ranges_km[cm][\"high\"]\n",
        "        if MODE in (\"Demo\",\"Interactive\"):\n",
        "            final_kcat_predictions=[\"skipped\"]*N; final_km_predictions=[\"skipped\"]*N\n",
        "            for j,k in enumerate(keep):\n",
        "                i0=valid[k]; final_kcat_predictions[i0]=int(yk[j]); final_km_predictions[i0]=int(ym[j])\n",
        "            kcat_inference_print=KcatInference(model_path=kcat_model_v1b_path,device=device,verbose=False)\n",
        "            km_inference_print=KcatInference(model_path=km_model_v1b_path,device=device,verbose=False)\n",
        "            print(\"\\n=== kcat Prediction Results ===\"); sys.stdout.flush()\n",
        "            kcat_inference_print.display_prediction_ranges_kcat(final_kcat_predictions,class_ranges_kcat); sys.stdout.flush()\n",
        "            print(\"\\n=== KM Prediction Results ===\"); sys.stdout.flush()\n",
        "            km_inference_print.display_prediction_ranges_km(final_km_predictions,class_ranges_km); sys.stdout.flush()\n",
        "# ---------------- write results ----------------\n",
        "# added section to add km predicted class numbers - by mariana\n",
        "df['Predicted_Kcat_class']=kcat_class_full\n",
        "# end of added section\n",
        "df['Predicted_Kcat_low']=kcat_low_full\n",
        "df['Predicted_Kcat_high']=kcat_high_full\n",
        "df['Predicted_Kcat_low (-1 class error)']=kcat_low_m1\n",
        "df['Predicted_Kcat_high (+1 class error)']=kcat_high_p1\n",
        "# added section to add km predicted class numbers - by mariana\n",
        "df['Predicted_KM_class']=km_class_full\n",
        "# end of added section\n",
        "df['Predicted_KM_low']=km_low_full\n",
        "df['Predicted_KM_high']=km_high_full\n",
        "df['Predicted_KM_low (-1 class error)']=km_low_m1\n",
        "df['Predicted_KM_high (+1 class error)']=km_high_p1\n",
        "\n",
        "df.to_csv(\"inference_results.csv\",index=False)\n",
        "print(\"Inference complete. Saved inference_results.csv\"); sys.stdout.flush()\n",
        "# try:\n",
        "#     if MODE==\"Bulk-large\" and IS_COLAB:\n",
        "#         _cf.download(\"inference_results.csv\")\n",
        "#     if MODE==\"Bulk\" and IS_COLAB and os.environ.get(\"DOWNLOAD_CSV\",\"\")==\"1\":\n",
        "#         _cf.download(\"inference_results.csv\")\n",
        "# except Exception as e:\n",
        "#     print(f\"[warn] Auto-download failed in subprocess: {e}\")\n",
        "PY\n",
        "# ---------------------------- BINARY BRANCH (ESM-2, v1-like) ----------------------------\n",
        "else\n",
        "  echo \"[RealKcat] Running Binary Alanine Simplified mode (optional, ESM-2)…\"\n",
        "  echo \"[setup] Installing system deps for Python 3.10 …\"\n",
        "  echo \"Trying to: Install dependencies, download and unzip model weights, get dependencies  [~2 minute]\"\n",
        "  sudo sed -i 's/^[[:space:]]*deb-src .*r2u\\.stat\\.illinois\\.edu.*$/# &/' /etc/apt/sources.list /etc/apt/sources.list.d/*.list 2>/dev/null || true\n",
        "  sudo apt-get update -qq > /dev/null\n",
        "  sudo apt-get install -qq -y python3.10 python3.10-distutils python3.10-venv curl wget unzip > /dev/null\n",
        "  curl -sS https://bootstrap.pypa.io/get-pip.py -o /tmp/get-pip.py\n",
        "  python3.10 /tmp/get-pip.py --quiet\n",
        "  python3.10 -m pip install -q numpy==1.23.5 pandas==1.5.3 scikit-learn==1.1.3 imbalanced-learn==0.8.1 matplotlib==3.6.3 seaborn==0.11.2 joblib==1.2.0 ipython==7.33.0 notebook==6.5.4 jupyterlab==3.6.1 openpyxl==3.1.2 xlrd==2.0.1 XlsxWriter==3.0.3\n",
        "  python3.10 -m pip install -q xgboost==2.1.4 torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 transformers==4.33.3 fair-esm==2.0.0 mkl==2022.1.0 mkl-service==2.4.0 intel-openmp==2022.1.0 tqdm\n",
        "    : > latex_queue.txt\n",
        "    MODE=\"$MODE\" CSV_PATH=\"$CSV_PATH\" BATCH_SIZE=\"$BATCH_SIZE\" stdbuf -oL -eL python3.10 - <<'PY'\n",
        "import os, sys, warnings, torch, argparse, random, math, joblib, numpy as np, pandas as pd, io, gc\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.serialization import add_safe_globals\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import esm\n",
        "os.environ[\"OMP_NUM_THREADS\"]=\"1\"; os.environ[\"MKL_NUM_THREADS\"]=\"1\"\n",
        "try:\n",
        "    torch.set_num_threads(1)\n",
        "except Exception:\n",
        "    pass\n",
        "MODE=os.environ.get(\"MODE\",\"Demo\").strip()\n",
        "CSV_PATH=os.environ[\"CSV_PATH\"]\n",
        "BATCH_SIZE=int(os.environ.get(\"BATCH_SIZE\",\"20\"))\n",
        "try:\n",
        "    from google.colab import files as _cf\n",
        "    IS_COLAB=True\n",
        "except Exception:\n",
        "    _cf=None; IS_COLAB=False\n",
        "def check_device(): return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device=check_device()\n",
        "print(\"Using device:\",device); sys.stdout.flush()\n",
        "print(sys.version); sys.stdout.flush()\n",
        "print(\"Starting inference setup…\"); sys.stdout.flush()\n",
        "MAX_SEQ_LENGTH=1022\n",
        "MAX_SMILES_LENGTH=512\n",
        "class_ranges_kcat={0:{\"low\":0.0,\"high\":3.32e-8},1:{\"low\":3.33e-8,\"high\":1.0e-2},2:{\"low\":1.01e-2,\"high\":1.0e-1},3:{\"low\":1.01e-1,\"high\":1.0},4:{\"low\":1.001,\"high\":10.0},5:{\"low\":1.004e1,\"high\":1.0e2},6:{\"low\":1.0025e2,\"high\":1.0e3},7:{\"low\":1.002e3,\"high\":7.0e7}}\n",
        "class_ranges_km={0:{\"low\":1.0e-10,\"high\":1.0e-5},1:{\"low\":1.01e-5,\"high\":1.0e-4},2:{\"low\":1.002e-4,\"high\":1.0e-3},3:{\"low\":1.002e-3,\"high\":1.0e-2},4:{\"low\":1.008e-2,\"high\":1.0e-1},5:{\"low\":1.01e-1,\"high\":1.02e2}}\n",
        "def format_sci(v):\n",
        "    s=f\"{v:.2e}\"\n",
        "    if \"e\" in s:\n",
        "        b,e=s.split(\"e\"); return f\"{b}x10^{int(e)}\"\n",
        "    return s\n",
        "warnings.filterwarnings(\"ignore\"); add_safe_globals([argparse.Namespace])\n",
        "df=pd.read_csv(CSV_PATH)\n",
        "df[\"sequence\"]=df[\"sequence\"].astype(str).str.replace(r\"\\s+\",\"\",regex=True).str.upper().str.replace(r\"[^ACDEFGHIKLMNPQRSTVWY]\", \"X\", regex=True)\n",
        "df[\"Isomeric SMILES\"]=df[\"Isomeric SMILES\"].astype(str).str.replace(r\"\\s+\",\"\",regex=True)\n",
        "pairs=list(zip(df[\"sequence\"],df[\"Isomeric SMILES\"]))\n",
        "def load_esm2_model(device,work_dir=\".\",verbose=True):\n",
        "    url=\"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\"\n",
        "    fn=url.split(\"/\")[-1]; p=os.path.join(work_dir,fn)\n",
        "    if not os.path.exists(p):\n",
        "        m=torch.hub.load_state_dict_from_url(url,progress=True,map_location=device); torch.save(m,p)\n",
        "    else:\n",
        "        if verbose: print(\"ESM2 model weights found locally. Loading from disk...\")\n",
        "        m=torch.load(p,map_location=device)\n",
        "    model,alphabet=esm.pretrained.load_model_and_alphabet_core(\"esm2_t33_650M_UR50D\",m)\n",
        "    model.eval().to(device); return model,alphabet\n",
        "class TensorDataset(Dataset):\n",
        "    def __init__(self,d,l): self.d,self.l=d,l\n",
        "    def __len__(self): return len(self.d)\n",
        "    def __getitem__(self,i): return self.d[i],self.l[i]\n",
        "def dataset_to_tensors(ds):\n",
        "    ld=DataLoader(ds,batch_size=len(ds),shuffle=False); return next(iter(ld))\n",
        "def standardize_x_global_separate(data,g1,s1,g2,s2):\n",
        "    X1,X2=data[:,:1280],data[:,1280:]; s1=torch.clamp(s1,min=1e-7); s2=torch.clamp(s2,min=1e-7)\n",
        "    return torch.cat(((X1-g1)/s1,(X2-g2)/s2),dim=1).squeeze(1)\n",
        "class StandardizedDatasetGlobalSeparate(Dataset):\n",
        "    def __init__(self,sub,g1,s1,g2,s2): self.sub=sub; self.g1=g1; self.s1=s1; self.g2=g2; self.s2=s2\n",
        "    def __len__(self): return len(self.sub)\n",
        "    def __getitem__(self,i):\n",
        "        x,y=self.sub[i]\n",
        "        if len(x.shape)==1: x=x.unsqueeze(1)\n",
        "        return standardize_x_global_separate(x,self.g1,self.s1,self.g2,self.s2),y\n",
        "def apply_global_standardization_separate(ds,g1,s1,g2,s2):\n",
        "    return StandardizedDatasetGlobalSeparate(ds,g1,s1,g2,s2)\n",
        "global_mean_1=torch.tensor(-0.0006011285004206002,device=device)\n",
        "global_std_1=torch.tensor(0.18902993202209473,device=device)\n",
        "global_mean_2=torch.tensor(-0.00015002528380136937,device=device)\n",
        "global_std_2=torch.tensor(0.6113553047180176,device=device)\n",
        "kcat_model_path=\"model_weights/kcat_model.pkl\"\n",
        "km_model_path=\"model_weights/km_model.pkl\"\n",
        "if not (os.path.exists(kcat_model_path) and os.path.exists(km_model_path)):\n",
        "    os.system(\"wget -q https://github.com/TKAI-LAB-Mali/RealKcat/raw/main/model_weights.zip -O model_weights.zip\")\n",
        "    os.system(\"unzip -o -q model_weights.zip\")\n",
        "_ESM=None; _ALPH=None; _TOK=None; _CHEM=None\n",
        "def _get_models():\n",
        "    global _ESM,_ALPH,_TOK,_CHEM\n",
        "    if _ESM is None or _ALPH is None:\n",
        "        _ESM,_ALPH=load_esm2_model(device,verbose=False)\n",
        "    if _TOK is None or _CHEM is None:\n",
        "        _TOK=AutoTokenizer.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k')\n",
        "        _CHEM=AutoModel.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k'); _CHEM.eval().to(device)\n",
        "    return _ESM,_ALPH,_TOK,_CHEM\n",
        "class KcatInference:\n",
        "    def __init__(self,model_path,device=None,verbose=True):\n",
        "        self.device=device if device else device\n",
        "        self.model=joblib.load(model_path); self.verbose=verbose\n",
        "        self.X_test_tensor=None; self.y1_test_tensor=None; self.keep_local=[]\n",
        "    def load_data_from_pairs(self,pairs):\n",
        "        embs=[]; keep=[]\n",
        "        esm_model,alphabet,tok,chem=_get_models(); bc=alphabet.get_batch_converter()\n",
        "        if self.verbose: print(\"Models loaded. Embedding sequences and substrates...\"); sys.stdout.flush()\n",
        "        for i,(seq,smi) in enumerate(pairs,1):\n",
        "            if not isinstance(seq,str) or not isinstance(smi,str): continue\n",
        "            if len(seq)>MAX_SEQ_LENGTH: continue\n",
        "            try:\n",
        "                _,_,bt=bc([(f\"sample_{i}\",seq)]); bt=bt.to(device); bl=(bt!=alphabet.padding_idx).sum(1)\n",
        "                with torch.no_grad(): r=esm_model(bt,repr_layers=[33],return_contacts=False)\n",
        "                es=r[\"representations\"][33][0,1:bl-1].mean(dim=0).float()\n",
        "            except Exception:\n",
        "                continue\n",
        "            try:\n",
        "                inp=tok([smi],return_tensors='pt',padding=True,truncation=False); inp={k:v.to(device) for k,v in inp.items()}\n",
        "                if inp['input_ids'].shape[1]>MAX_SMILES_LENGTH: continue\n",
        "                with torch.no_grad(): out=chem(**inp)\n",
        "                cs=out.last_hidden_state.mean(dim=1).squeeze(0).float()\n",
        "            except Exception:\n",
        "                continue\n",
        "            if es.dim()>1: es=es.flatten()\n",
        "            if cs.dim()>1: cs=cs.flatten()\n",
        "            embs.append(torch.cat((es,cs))); keep.append(i-1)\n",
        "        if not embs:\n",
        "            self.X_test_tensor=None; self.y1_test_tensor=None; self.keep_local=[]\n",
        "            return\n",
        "        self.X_test_tensor=torch.stack(embs).to(device); self.y1_test_tensor=torch.zeros(len(embs),dtype=torch.long).to(device); self.keep_local=keep\n",
        "    def standardize_test_data(self,g1,s1,g2,s2):\n",
        "        self.test_dataset_std=apply_global_standardization_separate(TensorDataset(self.X_test_tensor,self.y1_test_tensor),g1,s1,g2,s2) if self.X_test_tensor is not None else None\n",
        "    def convert_to_numpy(self):\n",
        "        if self.test_dataset_std is None: return None,None,[]\n",
        "        X,y=dataset_to_tensors(self.test_dataset_std); return X.cpu().numpy(),y.cpu().numpy(),self.keep_local\n",
        "    def predict(self,X): return self.model.predict(X)\n",
        "    def display_prediction_ranges_kcat(self,preds,cr):\n",
        "        for i,p in enumerate(preds):\n",
        "            if p is None or p==\"skipped\": print(f\"Sample {i+1}: skipped due to excessive length\")\n",
        "            else: print(f\"Sample {i+1}: Predicted Class = {p}, kcat range = [{format_sci(cr[p]['low'])}, {format_sci(cr[p]['high'])}]\")\n",
        "    def display_prediction_ranges_km(self,preds,cr):\n",
        "        for i,p in enumerate(preds):\n",
        "            if p is None or p==\"skipped\": print(f\"Sample {i+1}: skipped due to excessive length\")\n",
        "            else: print(f\"Sample {i+1}: Predicted Class = {p}, km range = [{format_sci(cr[p]['low'])}, {format_sci(cr[p]['high'])}]\")\n",
        "pairs_all=pairs\n",
        "valid=[i for i,(s,m) in enumerate(pairs_all) if isinstance(s,str) and isinstance(m,str) and len(s)<=MAX_SEQ_LENGTH and len(m)<=MAX_SMILES_LENGTH]\n",
        "if not valid:\n",
        "    df[\"Predicted_Kcat_low\"]=\"skipped\"; df[\"Predicted_Kcat_high\"]=\"skipped\"; df[\"Predicted_KM_low\"]=\"skipped\"; df[\"Predicted_KM_high\"]=\"skipped\"\n",
        "    df.to_csv(\"inference_results.csv\",index=False); print(\"Inference complete. Saved inference_results.csv\")\n",
        "    if MODE in (\"Bulk\",\"Bulk-large\") and IS_COLAB: _cf.download(\"inference_results.csv\")\n",
        "    sys.exit(0)\n",
        "def run_batch(idxs):\n",
        "    b=[pairs_all[i] for i in idxs]\n",
        "    inf=KcatInference(model_path=kcat_model_path,device=device,verbose=False)\n",
        "    inf.load_data_from_pairs(b)\n",
        "    if not inf.keep_local:\n",
        "        return [],[],[]\n",
        "    inf.standardize_test_data(global_mean_1,global_std_1,global_mean_2,global_std_2)\n",
        "    X,_,keep_local=inf.convert_to_numpy()\n",
        "    yk=KcatInference(model_path=kcat_model_path,device=device,verbose=False).predict(X)\n",
        "    ym=KcatInference(model_path=km_model_path,device=device,verbose=False).predict(X)\n",
        "    del inf, X; gc.collect()\n",
        "    return yk,ym,keep_local\n",
        "N=len(df)\n",
        "kcat_low_full=['skipped']*N; kcat_high_full=['skipped']*N; kcat_low_m1=['skipped']*N; kcat_high_p1=['skipped']*N\n",
        "km_low_full=['skipped']*N; km_high_full=['skipped']*N; km_low_m1=['skipped']*N; km_high_p1=['skipped']*N\n",
        "# added section for predicted class numbers - by mariana\n",
        "kcat_class_full=['skipped']*N\n",
        "km_class_full=['skipped']*N\n",
        "# end of added section\n",
        "max_kcat_c=max(class_ranges_kcat.keys()); max_km_c=max(class_ranges_km.keys())\n",
        "if MODE==\"Bulk-large\":\n",
        "    total_batches = math.ceil(len(valid)/BATCH_SIZE)\n",
        "    for b in tqdm(range(0, len(valid), BATCH_SIZE), desc=\"Batches\", unit=\"batch\", dynamic_ncols=True, file=sys.stdout, miniters=1):\n",
        "        idxs=valid[b:b+BATCH_SIZE]; yk,ym,keep=run_batch(idxs)\n",
        "        if not keep:\n",
        "            continue\n",
        "        for j,k in enumerate(keep):\n",
        "            i0=idxs[k]; ck=int(yk[j]); cm=int(ym[j])\n",
        "            # added section to store class numbers - by mariana\n",
        "            kcat_class_full[i0]=ck\n",
        "            km_class_full[i0]=cm\n",
        "            # end of added section\n",
        "            kcat_low_full[i0]=class_ranges_kcat[ck][\"low\"]; kcat_high_full[i0]=class_ranges_kcat[ck][\"high\"]\n",
        "            kcat_low_m1[i0]=class_ranges_kcat[max(ck-1,0)][\"low\"]; kcat_high_p1[i0]=class_ranges_kcat[min(ck+1,max_kcat_c)][\"high\"]\n",
        "            km_low_full[i0]=class_ranges_km[cm][\"low\"]; km_high_full[i0]=class_ranges_km[cm][\"high\"]\n",
        "            km_low_m1[i0]=class_ranges_km[max(cm-1,0)][\"low\"]; km_high_p1[i0]=class_ranges_km[min(cm+1,max_km_c)][\"high\"]\n",
        "        gc.collect(); sys.stdout.flush()\n",
        "else:\n",
        "    yk,ym,keep=run_batch(valid)\n",
        "    if keep:\n",
        "        for j,k in enumerate(keep):\n",
        "            i0=valid[k]; ck=int(yk[j]); cm=int(ym[j])\n",
        "            # added section to store class numbers - by mariana\n",
        "            kcat_class_full[i0]=ck\n",
        "            km_class_full[i0]=cm\n",
        "            # end of added section\n",
        "            kcat_low_full[i0]=class_ranges_kcat[ck][\"low\"]; kcat_high_full[i0]=class_ranges_kcat[ck][\"high\"]\n",
        "            km_low_full[i0]=class_ranges_km[cm][\"low\"]; km_high_full[i0]=class_ranges_km[cm][\"high\"]\n",
        "        if MODE in (\"Demo\",\"Interactive\"):\n",
        "            final_kcat_predictions=[\"skipped\"]*N; final_km_predictions=[\"skipped\"]*N\n",
        "            for j,k in enumerate(keep):\n",
        "                i0=valid[k]; final_kcat_predictions[i0]=int(yk[j]); final_km_predictions[i0]=int(ym[j])\n",
        "            kcat_inference_print=KcatInference(model_path=kcat_model_path,device=device,verbose=False)\n",
        "            km_inference_print=KcatInference(model_path=km_model_path,device=device,verbose=False)\n",
        "            print(\"\\n=== kcat Prediction Results ===\"); sys.stdout.flush()\n",
        "            kcat_inference_print.display_prediction_ranges_kcat(final_kcat_predictions,class_ranges_kcat); sys.stdout.flush()\n",
        "            print(\"\\n=== KM Prediction Results ===\"); sys.stdout.flush()\n",
        "            km_inference_print.display_prediction_ranges_km(final_km_predictions,class_ranges_km); sys.stdout.flush()\n",
        "# added section to add kcat predicted class numbers - by mariana\n",
        "df['Predicted_Kcat_class']=kcat_class_full\n",
        "# end of added section\n",
        "df['Predicted_Kcat_low']=kcat_low_full\n",
        "df['Predicted_Kcat_high']=kcat_high_full\n",
        "df['Predicted_Kcat_low (-1 class error)']=kcat_low_m1\n",
        "df['Predicted_Kcat_high (+1 class error)']=kcat_high_p1\n",
        "# added section to add km predicted class numbers - by mariana\n",
        "df['Predicted_KM_class']=km_class_full\n",
        "# end of added section\n",
        "df['Predicted_KM_low']=km_low_full\n",
        "df['Predicted_KM_high']=km_high_full\n",
        "df['Predicted_KM_low (-1 class error)']=km_low_m1\n",
        "df['Predicted_KM_high (+1 class error)']=km_high_p1\n",
        "df.to_csv(\"inference_results.csv\",index=False)\n",
        "print(\"Inference complete. Saved inference_results.csv\"); sys.stdout.flush()\n",
        "# try:\n",
        "#     if MODE==\"Bulk-large\" and IS_COLAB:\n",
        "#         _cf.download(\"inference_results.csv\")\n",
        "#     if MODE==\"Bulk\" and IS_COLAB and os.environ.get(\"DOWNLOAD_CSV\",\"\")==\"1\":\n",
        "#         _cf.download(\"inference_results.csv\")\n",
        "# except Exception as e:\n",
        "#     print(f\"[warn] Auto-download failed in subprocess: {e}\")\n",
        "PY\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s6rZPqoXtGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4eb3eb1-1a6a-4b9f-f656-3428f0f01a5d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_397286a2-af32-4168-a64b-adf8e3f2def1\", \"inference_results.csv\", 129605)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Download inference_results.csv to your computer\n",
        "import os\n",
        "from google.colab import files\n",
        "if os.path.exists(\"inference_results.csv\"):\n",
        "    files.download(\"inference_results.csv\")\n",
        "else:\n",
        "    print(\"inference_results.csv not found — run the cell above first.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "H100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}